import torch
from typing import Tuple, Dict, Any


class AudioDurationNode:
    """
    éŸ³é¢‘æ—¶é•¿è®¡ç®—èŠ‚ç‚¹
    è¾“å…¥AUDIOç±»å‹ï¼Œè¾“å‡ºè¯¥éŸ³é¢‘çš„æ—¶é•¿ï¼ˆç§’æ•°ï¼‰
    """

    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Dict[str, Any]]:
        return {
            "required": {
                "audio": ("AUDIO",),
            },
        }

    RETURN_TYPES = ("FLOAT", "STRING", "INT")
    RETURN_NAMES = ("duration_seconds", "formatted_time", "duration_ms")
    FUNCTION = "calculate_audio_duration"
    CATEGORY = "audio/utils"
    DESCRIPTION = "è®¡ç®—éŸ³é¢‘çš„æ—¶é•¿"
    OUTPUT_NODE = False

    def calculate_audio_duration(self, audio) -> Tuple[float, str, int]:
        """
        è®¡ç®—AUDIOç±»å‹çš„æ—¶é•¿

        Args:
            audio: AUDIOç±»å‹ï¼ˆå¯èƒ½æ˜¯Tensoræˆ–åŒ…å«Tensorçš„å­—å…¸ï¼‰
        """

        if audio is None:
            raise ValueError("éŸ³é¢‘è¾“å…¥ä¸èƒ½ä¸ºç©º")

        # æå–éŸ³é¢‘å¼ é‡å’Œé‡‡æ ·ç‡
        audio_tensor, sample_rate = self._extract_audio_info(audio)

        if audio_tensor is None:
            raise ValueError("æ— æ³•ä»AUDIOè¾“å…¥ä¸­æå–æœ‰æ•ˆçš„éŸ³é¢‘å¼ é‡")

        # è·å–éŸ³é¢‘å½¢çŠ¶ä¿¡æ¯
        original_shape = audio_tensor.shape

        # å¤„ç†ä¸åŒçš„éŸ³é¢‘å¼ é‡æ ¼å¼
        if len(original_shape) == 1:
            # [samples] -> å•å£°é“
            num_samples = original_shape[0]
        elif len(original_shape) == 2:
            # [channels, samples]
            num_samples = original_shape[1]
        elif len(original_shape) == 3:
            # [batch, channels, samples] -> æ ‡å‡†æ ¼å¼
            num_samples = original_shape[2]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„éŸ³é¢‘å¼ é‡ç»´åº¦: {len(original_shape)}")

        # è®¡ç®—æ—¶é•¿
        duration_seconds = num_samples / sample_rate

        # æ ¼å¼åŒ–æ—¶é—´è¾“å‡º (HH:MM:SS.mmm)
        hours = int(duration_seconds // 3600)
        minutes = int((duration_seconds % 3600) // 60)
        seconds = int(duration_seconds % 60)
        milliseconds = int((duration_seconds - int(duration_seconds)) * 1000)

        formatted_time = f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"
        duration_ms = int(duration_seconds * 1000)

        # è¾“å‡ºä¿¡æ¯
        print(f"éŸ³é¢‘æ—¶é•¿è®¡ç®— - é‡‡æ ·æ•°: {num_samples}, é‡‡æ ·ç‡: {sample_rate}Hz")
        print(f"æ—¶é•¿: {formatted_time} ({duration_seconds:.3f}ç§’)")

        return (duration_seconds, formatted_time, duration_ms)

    def _extract_audio_info(self, audio):
        """ä»AUDIOç±»å‹ä¸­æå–éŸ³é¢‘å¼ é‡å’Œé‡‡æ ·ç‡"""
        audio_tensor = None
        sample_rate = 44100  # é»˜è®¤é‡‡æ ·ç‡

        if isinstance(audio, torch.Tensor):
            # ç›´æ¥æ˜¯Tensor
            audio_tensor = audio

        elif isinstance(audio, dict):
            # å­—å…¸ç»“æ„ï¼Œå°è¯•æå–éŸ³é¢‘å¼ é‡
            possible_tensor_keys = ['samples', 'audio', 'data', 'waveform', 'tensor']

            for key in possible_tensor_keys:
                if key in audio and isinstance(audio[key], torch.Tensor):
                    audio_tensor = audio[key]
                    break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¸¸è§é”®åï¼Œå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªTensorå€¼
            if audio_tensor is None:
                for value in audio.values():
                    if isinstance(value, torch.Tensor):
                        audio_tensor = value
                        break

            # å°è¯•æå–é‡‡æ ·ç‡
            possible_sr_keys = ['sample_rate', 'sampling_rate', 'sr', 'rate']
            for key in possible_sr_keys:
                if key in audio and isinstance(audio[key], (int, float)):
                    sample_rate = audio[key]
                    break

        return audio_tensor, sample_rate


class AudioSimpleDuration:
    """
    ç®€åŒ–ç‰ˆéŸ³é¢‘æ—¶é•¿è®¡ç®—ï¼ˆåªè¾“å‡ºç§’æ•°ï¼‰
    """

    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Dict[str, Any]]:
        return {
            "required": {
                "audio": ("AUDIO",),
            },
        }

    RETURN_TYPES = ("FLOAT",)
    RETURN_NAMES = ("duration_seconds",)
    FUNCTION = "calculate_simple_duration"
    CATEGORY = "audio/utils"
    DESCRIPTION = "è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆç®€åŒ–ç‰ˆï¼‰"

    def calculate_simple_duration(self, audio) -> Tuple[float]:
        """ç®€åŒ–ç‰ˆæ—¶é•¿è®¡ç®—"""
        if audio is None:
            raise ValueError("éŸ³é¢‘è¾“å…¥ä¸èƒ½ä¸ºç©º")

        audio_tensor, sample_rate = self._extract_audio_info(audio)

        if audio_tensor is None:
            raise ValueError("æ— æ³•æå–éŸ³é¢‘å¼ é‡")

        shape = audio_tensor.shape
        if len(shape) == 3:
            num_samples = shape[2]
        elif len(shape) == 2:
            num_samples = shape[1]
        elif len(shape) == 1:
            num_samples = shape[0]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„éŸ³é¢‘ç»´åº¦: {len(shape)}")

        duration_seconds = num_samples / sample_rate
        return (duration_seconds,)

    def _extract_audio_info(self, audio):
        """æå–éŸ³é¢‘ä¿¡æ¯"""
        audio_tensor = None
        sample_rate = 44100

        if isinstance(audio, torch.Tensor):
            audio_tensor = audio
        elif isinstance(audio, dict):
            for key in ['samples', 'audio', 'data']:
                if key in audio and isinstance(audio[key], torch.Tensor):
                    audio_tensor = audio[key]
                    break

            for key in ['sample_rate', 'sampling_rate']:
                if key in audio and isinstance(audio[key], (int, float)):
                    sample_rate = audio[key]
                    break

        return audio_tensor, sample_rate


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "AudioDuration": AudioDurationNode,
    "AudioDurationSimple": AudioSimpleDuration,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AudioDuration": "ğŸ”Š Audio Duration",
    "AudioDurationSimple": "ğŸ”Š Audio Duration (Simple)",
}